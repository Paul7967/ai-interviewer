# Ollama Integration

–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Ollama —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π GPU –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ AI Interviewer.

## üöÄ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ GPU (NVIDIA CUDA)
- ‚úÖ API –¥–æ—Å—Ç—É–ø —á–µ—Ä–µ–∑ HTTP
- ‚úÖ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª—è–º–∏
- ‚úÖ –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
- ‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤
- ‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å FastAPI backend

## üìã –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

- Docker —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π NVIDIA Container Runtime
- NVIDIA GPU —Å –¥—Ä–∞–π–≤–µ—Ä–∞–º–∏ CUDA
- Docker Compose

## üõ†Ô∏è –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –∑–∞–ø—É—Å–∫

### 1. –ó–∞–ø—É—Å–∫ —Ç–æ–ª—å–∫–æ Ollama

```bash
# –ó–∞–ø—É—Å–∫ Ollama —Å GPU –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
docker-compose -f docker-compose.ollama.yml up -d

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞
./scripts/ollama-manager.sh status
```

### 2. –ó–∞–ø—É—Å–∫ —Å –æ—Å–Ω–æ–≤–Ω—ã–º –ø—Ä–æ–µ–∫—Ç–æ–º

```bash
# –ó–∞–ø—É—Å–∫ –≤—Å–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ –≤–∫–ª—é—á–∞—è Ollama
docker-compose up -d

# –ò–ª–∏ —Ç–æ–ª—å–∫–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã
docker-compose up -d postgres backend ollama
```

### 3. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —Å–∫—Ä–∏–ø—Ç

```bash
# –ó–∞–ø—É—Å–∫
./scripts/ollama-manager.sh start

# –û—Å—Ç–∞–Ω–æ–≤–∫–∞
./scripts/ollama-manager.sh stop

# –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫
./scripts/ollama-manager.sh restart

# –°—Ç–∞—Ç—É—Å
./scripts/ollama-manager.sh status

# –õ–æ–≥–∏
./scripts/ollama-manager.sh logs
```

## üì¶ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª—è–º–∏

### –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π

```bash
# –ó–∞–≥—Ä—É–∑–∫–∞ Code Llama
./scripts/ollama-manager.sh pull codellama:latest

# –ó–∞–≥—Ä—É–∑–∫–∞ Stable Code
./scripts/ollama-manager.sh pull stable-code:3b-code-q4_0

# –ó–∞–≥—Ä—É–∑–∫–∞ Llama 2
./scripts/ollama-manager.sh pull llama2:latest
```

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

```bash
# –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏ —Å –ø—Ä–æ–º–ø—Ç–æ–º
./scripts/ollama-manager.sh run codellama "–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞ Python –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –º–∞—Å—Å–∏–≤–∞"

# –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
./scripts/ollama-manager.sh run codellama
```

## üîå API Endpoints

Ollama API –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ `http://localhost:11434`

### –û—Å–Ω–æ–≤–Ω—ã–µ endpoints:

- `GET /api/tags` - –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
- `POST /api/pull` - –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
- `POST /api/generate` - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
- `POST /api/chat` - –ß–∞—Ç —Å –º–æ–¥–µ–ª—å—é
- `POST /api/embeddings` - –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤

### –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤:

```bash
# –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
curl http://localhost:11434/api/tags

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "codellama:latest",
    "prompt": "–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞ Python",
    "stream": false
  }'

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
curl -X POST http://localhost:11434/api/pull \
  -H "Content-Type: application/json" \
  -d '{"name": "codellama:latest"}'
```

## üêç Python Integration

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ FastAPI

```python
from ollama_client import OllamaClient, OllamaGenerateRequest

# –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
async def generate_code(prompt: str):
    async with OllamaClient() as client:
        request = OllamaGenerateRequest(
            model="codellama:latest",
            prompt=prompt,
            stream=False
        )
        response = await client.generate(request)
        return response.response

# –°–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
from ollama_client import OllamaClientSync

client = OllamaClientSync()
models = client.list_models()
```

### –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º

```python
# –í main.py –¥–æ–±–∞–≤—å—Ç–µ:
from ollama_client import OllamaClient

@app.get("/ollama/models")
async def get_ollama_models():
    async with OllamaClient() as client:
        if await client.health_check():
            models = await client.list_models()
            return {"models": [model.dict() for model in models]}
        else:
            raise HTTPException(status_code=503, detail="Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

@app.post("/ollama/generate")
async def generate_with_ollama(prompt: str, model: str = "codellama:latest"):
    async with OllamaClient() as client:
        request = OllamaGenerateRequest(
            model=model,
            prompt=prompt,
            stream=False
        )
        response = await client.generate(request)
        return {"response": response.response}
```

## üîß –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

### –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è

```bash
# Ollama –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
CUDA_VISIBLE_DEVICES=0          # GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
OLLAMA_HOST=0.0.0.0            # –•–æ—Å—Ç –¥–ª—è API
OLLAMA_ORIGINS=*               # CORS –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
OLLAMA_DEBUG=INFO              # –£—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
```

### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GPU

```bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
nvidia-smi

# –ü—Ä–æ–≤–µ—Ä–∫–∞ Docker GPU –ø–æ–¥–¥–µ—Ä–∂–∫–∏
docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi
```

## üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

### GPU –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

```bash
# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
docker exec ai-interviewer-ollama nvidia-smi -l 1

# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–º—è—Ç–∏
docker exec ai-interviewer-ollama nvidia-smi --query-gpu=memory.used,memory.total --format=csv
```

### –õ–æ–≥–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞

```bash
# –ü—Ä–æ—Å–º–æ—Ç—Ä –ª–æ–≥–æ–≤
docker logs ai-interviewer-ollama -f

# –ò–ª–∏ —á–µ—Ä–µ–∑ —Å–∫—Ä–∏–ø—Ç
./scripts/ollama-manager.sh logs
```

## üö® Troubleshooting

### –ü—Ä–æ–±–ª–µ–º—ã —Å GPU

1. **GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω:**
   ```bash
   # –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥—Ä–∞–π–≤–µ—Ä—ã
   nvidia-smi
   
   # –ü—Ä–æ–≤–µ—Ä—å—Ç–µ Docker GPU –ø–æ–¥–¥–µ—Ä–∂–∫—É
   docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi
   ```

2. **–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ GPU:**
   ```bash
   # –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–æ–¥–µ–ª–∏ —Å –º–µ–Ω—å—à–∏–º —Ä–∞–∑–º–µ—Ä–æ–º
   ./scripts/ollama-manager.sh pull stable-code:3b-code-q4_0
   ```

### –ü—Ä–æ–±–ª–µ–º—ã —Å API

1. **API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω:**
   ```bash
   # –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç–∞—Ç—É—Å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
   docker ps | grep ollama
   
   # –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
   ./scripts/ollama-manager.sh restart
   ```

2. **–û—à–∏–±–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è:**
   ```bash
   # –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ—Ä—Ç
   netstat -tlnp | grep 11434
   
   # –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏
   ./scripts/ollama-manager.sh logs
   ```

## üìà –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è RTX 4060 (8GB VRAM):

- `codellama:latest` - 3.8GB
- `stable-code:3b-code-q4_0` - 1.6GB
- `llama2:7b-q4_0` - 4GB
- `mistral:7b-q4_0` - 4GB

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è:

```bash
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
./scripts/ollama-manager.sh pull stable-code:3b-code-q4_0

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "codellama:latest",
    "prompt": "–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é",
    "options": {
      "num_predict": 100,
      "temperature": 0.7
    }
  }'
```

## üîó –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏

- [Ollama Documentation](https://ollama.ai/docs)
- [Ollama API Reference](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [NVIDIA Container Runtime](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)
- [Docker GPU Support](https://docs.docker.com/config/containers/resource_constraints/#gpu)
