# üöÄ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Ollama —Å GPU –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π - –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç

## ‚úÖ –ß—Ç–æ –±—ã–ª–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ

### 1. Docker –æ–±—Ä–∞–∑ —Å Ollama –∏ GPU –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
- **–û–±—Ä–∞–∑:** `ollama/ollama:latest` —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π NVIDIA CUDA
- **GPU:** NVIDIA GeForce RTX 4060 Laptop GPU (8GB VRAM)
- **CUDA:** –í–µ—Ä—Å–∏—è 12.2
- **–°—Ç–∞—Ç—É—Å:** ‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ

### 2. Docker Compose –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
- **–§–∞–π–ª—ã –æ–±–Ω–æ–≤–ª–µ–Ω—ã:**
  - `docker-compose.yml` (development)
  - `docker-compose.prod.yml` (production)
- **–ü–æ—Ä—Ç—ã:** 11434 (Ollama API)
- **Volumes:** 
  - `ollama_models` - –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
  - `ollama_data` - –¥–ª—è –¥–∞–Ω–Ω—ã—Ö Ollama
- **GPU –ø–æ–¥–¥–µ—Ä–∂–∫–∞:** –ù–∞—Å—Ç—Ä–æ–µ–Ω–∞ —á–µ—Ä–µ–∑ `deploy.resources.reservations.devices`

### 3. –°–∫—Ä–∏–ø—Ç —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è Ollama
- **–§–∞–π–ª:** `scripts/ollama-manager.sh`
- **–§—É–Ω–∫—Ü–∏–∏:**
  - `start` - –∑–∞–ø—É—Å–∫ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
  - `stop` - –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
  - `restart` - –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫
  - `status` - –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞
  - `pull <model>` - –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
  - `run <model> [prompt]` - –∑–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏
  - `logs` - –ø—Ä–æ—Å–º–æ—Ç—Ä –ª–æ–≥–æ–≤

### 4. Python –∫–ª–∏–µ–Ω—Ç –¥–ª—è Ollama API
- **–§–∞–π–ª:** `backend_fastapi/ollama_client.py`
- **–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
  - –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –≤—ã–∑–æ–≤—ã
  - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
  - –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
  - –ß–∞—Ç —Å –º–æ–¥–µ–ª—è–º–∏
  - –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
  - –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª—è–º–∏

### 5. –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- **–§–∞–π–ª:** `ollama/README.md` - –ø–æ–¥—Ä–æ–±–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- **–°–æ–¥–µ—Ä–∂–∏—Ç:** –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ —É—Å—Ç–∞–Ω–æ–≤–∫–µ, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é, troubleshooting

## üñ•Ô∏è –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏

### –ê–ø–ø–∞—Ä–∞—Ç–Ω–æ–µ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ
- **GPU:** NVIDIA GeForce RTX 4060 Laptop GPU
- **VRAM:** 8.0 GiB (–¥–æ—Å—Ç—É–ø–Ω–æ 6.9 GiB)
- **CUDA:** 12.2
- **Compute Capability:** 8.9

### –ü—Ä–æ–≥—Ä–∞–º–º–Ω–æ–µ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ
- **Ollama:** –í–µ—Ä—Å–∏—è 0.11.8
- **Docker:** 27.0.3
- **NVIDIA Container Runtime:** ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
- **CUDA –±–∏–±–ª–∏–æ—Ç–µ–∫–∞:** v12

## üì¶ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏

### –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:
1. **codellama:latest** (3.8GB)
   - –°–µ–º–µ–π—Å—Ç–≤–æ: Llama
   - –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: 7B
   - –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è: Q4_0
   - –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞

2. **stable-code:3b-code-q4_0** (1.6GB)
   - –°–µ–º–µ–π—Å—Ç–≤–æ: StableLM
   - –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: 3B
   - –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è: Q4_0
   - –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞

## üîß –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

### –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
```bash
CUDA_VISIBLE_DEVICES=0          # GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
OLLAMA_HOST=0.0.0.0            # –•–æ—Å—Ç –¥–ª—è API
OLLAMA_ORIGINS=*               # CORS –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
OLLAMA_DEBUG=INFO              # –£—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
```

### Healthcheck
```yaml
healthcheck:
  test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:11434/api/tags"]
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 60s
```

## üöÄ –ö–æ–º–∞–Ω–¥—ã –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –ó–∞–ø—É—Å–∫ Ollama
```bash
# –ó–∞–ø—É—Å–∫ —Ç–æ–ª—å–∫–æ Ollama
docker-compose up -d ollama

# –ó–∞–ø—É—Å–∫ –≤—Å–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞
docker-compose up -d

# –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —Å–∫—Ä–∏–ø—Ç
./scripts/ollama-manager.sh start
```

### –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª—è–º–∏
```bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞
./scripts/ollama-manager.sh status

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
./scripts/ollama-manager.sh pull codellama:latest

# –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏
./scripts/ollama-manager.sh run stable-code:3b-code-q4_0 "–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞ Python"
```

### API –∑–∞–ø—Ä–æ—Å—ã
```bash
# –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
curl http://localhost:11434/api/tags

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "stable-code:3b-code-q4_0", "prompt": "–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é", "stream": false}'
```

## üêç Python –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è

### –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
```python
from ollama_client import OllamaClient, OllamaGenerateRequest

async def generate_code(prompt: str):
    async with OllamaClient() as client:
        request = OllamaGenerateRequest(
            model="stable-code:3b-code-q4_0",
            prompt=prompt,
            stream=False
        )
        response = await client.generate(request)
        return response.response
```

### –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å FastAPI
```python
@app.post("/ollama/generate")
async def generate_with_ollama(prompt: str, model: str = "stable-code:3b-code-q4_0"):
    async with OllamaClient() as client:
        request = OllamaGenerateRequest(
            model=model,
            prompt=prompt,
            stream=False
        )
        response = await client.generate(request)
        return {"response": response.response}
```

## üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

### GPU –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
```bash
# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ GPU
docker exec ai-interviewer-ollama nvidia-smi

# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
docker exec ai-interviewer-ollama nvidia-smi -l 1
```

### –õ–æ–≥–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
```bash
# –ü—Ä–æ—Å–º–æ—Ç—Ä –ª–æ–≥–æ–≤
./scripts/ollama-manager.sh logs

# –ò–ª–∏ –Ω–∞–ø—Ä—è–º—É—é
docker logs ai-interviewer-ollama -f
```

## ‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

### –¢–µ—Å—Ç 1: –ó–∞–ø—É—Å–∫ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
- **–°—Ç–∞—Ç—É—Å:** ‚úÖ –£—Å–ø–µ—à–Ω–æ
- **GPU –æ–±–Ω–∞—Ä—É–∂–µ–Ω:** ‚úÖ –î–∞
- **API –¥–æ—Å—Ç—É–ø–µ–Ω:** ‚úÖ –î–∞
- **Healthcheck:** ‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç

### –¢–µ—Å—Ç 2: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
- **–ú–æ–¥–µ–ª—å:** stable-code:3b-code-q4_0
- **–†–∞–∑–º–µ—Ä:** 1.6GB
- **–°—Ç–∞—Ç—É—Å:** ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ
- **–í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏:** ~2-3 –º–∏–Ω—É—Ç—ã

### –¢–µ—Å—Ç 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞
- **–ú–æ–¥–µ–ª—å:** stable-code:3b-code-q4_0
- **–ü—Ä–æ–º–ø—Ç:** "–ù–∞–ø–∏—à–∏ –ø—Ä–æ—Å—Ç—É—é —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞ Python –¥–ª—è —Å–ª–æ–∂–µ–Ω–∏—è –¥–≤—É—Ö —á–∏—Å–µ–ª"
- **–†–µ–∑—É–ª—å—Ç–∞—Ç:** ‚úÖ –ö–æ–¥ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω
- **–í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞:** ~3 —Å–µ–∫—É–Ω–¥—ã

### –¢–µ—Å—Ç 4: GPU –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
- **VRAM –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ:** ~2.2GB
- **VRAM –¥–æ—Å—Ç—É–ø–Ω–æ:** 6.9GB –∏–∑ 8GB
- **–£—Ç–∏–ª–∏–∑–∞—Ü–∏—è GPU:** 15-40%
- **–°—Ç–∞—Ç—É—Å:** ‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω–æ

## üéØ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏

1. **–õ–æ–∫–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞** - –Ω–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–Ω–µ—à–Ω–∏—Ö API
2. **GPU —É—Å–∫–æ—Ä–µ–Ω–∏–µ** - –±—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é CUDA
3. **–ì–∏–±–∫–æ—Å—Ç—å** - –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤—ã–±–æ—Ä–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
4. **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å** - –ª–µ–≥–∫–æ –¥–æ–±–∞–≤–ª—è—Ç—å –Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
5. **–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è** - –≥–æ—Ç–æ–≤—ã–π Python –∫–ª–∏–µ–Ω—Ç –¥–ª—è backend
6. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥** - –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è

## üîÆ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é

### –î–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `stable-code:3b-code-q4_0` –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `codellama:latest` –¥–ª—è –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á
- –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥ –≤–∞—à–∏ –Ω—É–∂–¥—ã

### –î–ª—è production
- –û–≥—Ä–∞–Ω–∏—á—å—Ç–µ –ø–∞–º—è—Ç—å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞: `memory: 8G`
- –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ healthcheck –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞
- –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–æ–¥–µ–ª–∏ —Å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π Q4_0 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
- –ù–∞—Å—Ç—Ä–æ–π—Ç–µ `OLLAMA_NUM_PARALLEL` –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
- –ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ VRAM

## üéâ –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Ollama —Å GPU –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –¥–ª—è:

- –ì–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞
- –ê–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞
- –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
- –û–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

–í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ, GPU –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ, –∏ —Å–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –æ—Å–Ω–æ–≤–Ω—ã–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º AI Interviewer.
